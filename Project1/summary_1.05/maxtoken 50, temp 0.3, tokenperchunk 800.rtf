{\rtf1\ansi\ansicpg949\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
[', sometimes referred to as intra-attention [14], has been used in a variety of tasks such as reading comprehension [22], abstractive summarization [30] and translation [27]. In this work we generalize the concept of self-', ' factor of \uc0\u8730 dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice,', " the pre-softmax linear transformation, similar to [2].\\n\\nThis paper titled 'Attention is all you need' discusses the use of attention mechanisms in the Transformer model, a neural network architecture for language understanding. It explains how multi", ' O (logk (n)) convolutional layers, which is still much faster than a recurrent layer. However, the maximum path length between any two positions is O (logk (n)), which is much longer than for self-attention', ', at a fraction of the training cost of any of the competitive models. The Transformer also outperforms the best previously reported models on the WMT 2014 English-to-French translation task, establishing a new state-of-the-art B', ' and observe a small improvement.\\n\\nThis paper presents the Transformer model, which outperforms all previously published single models on the WMT 2014 English-to-French translation task, achieving a BLEU score of 41.0. It is', '\\n\\nThis paper presents the Transformer, a sequence transduction model based entirely on attention, which replaces the recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention. Experiments on English constituency parsing and W']\
}