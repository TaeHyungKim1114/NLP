{\rtf1\ansi\ansicpg949\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 1)\
\
This paper presents the Transformer, a sequence transduction model based entirely on self-attention mechanisms. It is faster and more space-efficient than architectures based on recurrent or convolutional layers, and achieves state-of-the-art results on two machine translation tasks. The Transformer was evaluated with different variations to determine the importance of its components, showing that dropout and multi-head attention are important for its performance. Overall, this paper demonstrates the efficacy of self-attention as a general-purpose mechanism for sequence transduction tasks.\
\
2)\
\
This paper presents the Transformer, a sequence transduction model based entirely on attention, which replaces the recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention. It was tested on the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, and achieved a new state of the art in both tasks, outperforming all previously reported ensembles. It was also tested on English constituency parsing and observed nearly identical performance to the best reported models. The Transformer model is computationally more efficient than recurrent models, and can be trained in less than a day on a single GPU.\
\
3)\
\
The Transformer is shown to be computationally more efficient than recurrent or convolutional architectures, and is able to learn long-range dependencies with a constant number of parameters. The paper also presents a novel application of the Transformer to semi-supervised learning, in which the model is trained on unlabeled data in addition to labeled data. Experiments on the WMT 2014 English-to-French translation task show that the Transformer can be trained with significantly less labeled data than the base model, while still achieving the same level of performance.\
\
4)\
\
This paper presents the Transformer, a sequence transduction model based entirely on attention, which replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. This model was tested on English-to-German and English-to-French translation tasks and achieved a new state of the art. Additionally, when applied to English constituency parsing, the Transformer outperformed all previously reported models with the exception of the Recurrent Neural Network Grammar.\
 The model was also significantly more efficient than the previous models, requiring only a fraction of the training cost.\
\
5)\
\
This paper presents the Transformer, a novel sequence-to-sequence model that replaces convolutional and recurrent layers with multi-headed self-attention layers. This model was tested on several tasks such as machine translation, constituency parsing, and language modelling and outperformed existing models on these tasks, while requiring fewer training steps and less computation. It was also able to generalize well to other tasks.\
\
\
\
\
\
}